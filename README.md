# annotation-tool

A lightweight web-based annotation tool for labelling entity recognition data.

The tool was designed with rapid annotation in mind. Unlike other annotation tools, this one is keyboard-based, allowing users to tag entities much faster than with the mouse. 

## Dependencies

- Python 2.7
	- [NLTK](http://www.nltk.org/) (optional but highly recommended)
	- [Colorama](https://pypi.python.org/pypi/colorama) (optional)
- Google Chrome or Mozilla Firefox (latest versions recommended. Tested in September 2017).

## How to use

There are three steps to using this annotation tool:

1. Generating the Javascript file for use by the web-based interface.
2. Annotating the data through the web-based interface.
3. Merging the annotated data into one file.

### 1. Generating the Javascript file

The web-based interface uses a Javascript file, `json_data.js`, to load in the sentences and entity categories. Before using the web-based interface, you must first generate this file.

To begin, you'll need to place two files in the root directory of this repository:

1. `raw_data.txt`, which contains all of the sentences in your dataset, separated by newline characters, and
2. `entity_classes.txt`, which contains your entity categories. Each line comprises a category name, followed by a tab character, followed by the abbreviated form of that category.

Note: at this stage, up to 10 entity classes are supported.

Examples of `raw_data.txt` and `entity_classes.txt` can be found in the `examples` folder. The example dataset was taken from the Gutenberg corpus, found in the NLTK library.

Once you've placed the two files into the root directory, you may run the following to generate the `json_data.js` file:

    python generate_js.py
    
This script will tokenise all of your data using NLTK, and will convert all of the data into Javascript arrays.
    
### 2. Annotating the data through the web-based interface

You may now successfully open `tagging.html` in your browser.

Before commencing with the annotation, you may wish to change your download directory to a new folder, as the `annotated_data.txt` files generated by the tool will fill your download directory as you annotate your data.

The sentences are separated into groups of 10. Whenever you complete the annotation of one group, a text file such as `annotated_data_n` (where `n` is the group) will be saved to your download directory. This way, you may close the webpage at any time and resume annotating later.

You can also manually save the current group by pressing the Save button in the top-right.

Note: I've found Chrome to be faster than Firefox for this application as it doesn't ask for confirmation before downloading each file.

#### Controls

- Left/right arrow: Select previous/next token
- Up/down arrow: Select previous/next sentence
- Shift + left/right arrow: Select multiple tokens at once
- 1-9, 0: Tag token with the corresponding class
- j: Jump to group

### 3. Merging the annotated data into one file

Once you've completed annotating, there will be a bunch of `annotated_data.txt` files in your download directory. Create a folder named `annotated_data` in the root directory of this repository, and copy/paste all of the annotated data files into it.

You can then run the `combine_data.py` program to combine them all into one file:

    python combine_data.py
    
This program will only merge the most recent versions of each annotated file, so if you tagged the same group twice (and have something like `annotated_data_1.txt` and `annotated_data_1 (1).txt`), it will place the old versions into a folder named `old_versions`.

Once the program has finished, you can find your merged file inside the `annotated_data/merged_data` folder. The format of each line in the file is a token, followed by a space, followed by its entity class. For example,

    The O
    person B-PER
    was O
    waiting O
    inside O
    the O
    Apple B-ORG
    shop O

Please let me know if there are any issues!
